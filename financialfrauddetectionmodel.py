# -*- coding: utf-8 -*-
"""FinancialFraudDetectionModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ICHno5yqFv6AovnfGfPFBeF1n-ktTad

## Importing Libraries
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import plotly.express as px

"""## Reading Data"""

data = pd.read_csv('/content/sample_data/fraud_detection.csv')

data.head(5)

"""### Obtaining data info"""

data.info()

data.shape

"""## Data Cleaning

### Checking for missing values
"""

msno.bar(data)

"""#### All columns show 100% completed data records!

### Number of duplicates in data
"""

data.duplicated().sum()

"""### Renaming columns"""

data = data.rename(columns = {'nameOrig':'origin', 'oldbalanceOrg':'sender_old_bal', 'newbalanceOrig':'sender_new_bal', 'nameDest':'dest', 'oldbalanceDest':'dest_old_bal', 'newbalanceDest':'dest_new_bal'})

"""### Removing non-essential columns"""

data = data.drop(columns = ['step', 'isFlaggedFraud'], axis='columns')

"""### Viewing modified data"""

data.head()

"""## Data Analysis

### Visualizing fraud occurances
"""

plt.figure(figsize=(15,8))
g = sns.countplot(data=data, x='type', hue='isFraud', palette='dark',alpha=0.6)
plt.title("Checking for fraud occurances for each Transaction types")

#finding counts
for p in g.patches:
    g.annotate(p.get_height(), (p.get_x()+0.04, p.get_height()+10000))



"""### Accounts involved in fraud transactions

#### Checking origin accounts (and if same account has been used for multiple fraudulent transactions)

##### For type 'Transfer'
"""

transfer_fraud = data[(data['type']=='TRANSFER') & (data['isFraud']==1)]

transfer_fraud['origin'].value_counts()

"""##### For type 'CashOut'"""

cashOut_fraud = data[(data['type']=='CASH_OUT') & (data['isFraud']==1)]

cashOut_fraud['origin'].value_counts()

"""#### Checking destination accounts (and if same account has been used for multiple fraudulent transactions)

##### For type 'Transfer'
"""

transfer_fraud['dest'].value_counts()

"""##### For type 'CashOut'"""

cashOut_fraud['dest'].value_counts()

"""### Checking if transfer destination account used for cashing out (for fraud cases)"""

transfer_fraud.dest.isin(cashOut_fraud.origin).any()

trans_cashout = data[data['type']=='TRANSFER']

transfer_fraud.dest.isin(trans_cashout.origin).any()

transfer_fraud.dest.isin(trans_cashout.origin).count()

"""### Insights

<ul>
    <li>Fraud Transactions are seen only for 'TRANSFER' (4097 occurances) and 'CASH_OUT' (4116 occurances) types</li>
    <li>Fraud tranasactions are usually to and from customers</li>
    <li>Recipient accounts fraud 'TRANSFER' cases were -</li>
    <ul>
        <li>not involved in any fraudulent cash withdrawals</li>
        <li>4097 times used to make valid cash withdrawals</li>
    </ul>
</ul>

## Feature Engineering

### Creating new column client_type having 4 categories and dropping 'origin' and 'dest'

<ul>
    <li>CC - Customer to Customer</li>
    <li>CM - Customer to Merchant</li>
    <li>MC - Merchant to Customer</li>
    <li>MM - Merchant to Merchant</li>
</ul>

#### Adding 'client_type' column
"""

data['client_type'] = np.nan
data.loc[data.origin.str.contains('C') & data.dest.str.contains('C'), 'client_type'] = 'CC'
data.loc[data.origin.str.contains('C') & data.dest.str.contains('M'), 'client_type'] = 'CM'
data.loc[data.origin.str.contains('M') & data.dest.str.contains('C'), 'client_type'] = 'MC'
data.loc[data.origin.str.contains('M') & data.dest.str.contains('M'), 'client_type'] = 'MM'

"""#### Dropping 'origin' and 'dest'"""

data.drop(columns = ['origin', 'dest'], axis = 'columns', inplace = True)

data.head()

"""## Data Visualization

### Obtaining number of fraud transactions and valid transactions of each 'client_type'
"""

fraudTrans = data[data['isFraud'] == 1]
validTrans = data[data['isFraud'] == 0]

fr = fraudTrans.client_type.value_counts()
print(fr)

va = validTrans.client_type.value_counts()
print(va)

"""### Visualizing the fraud and valid transaction counts for each 'client_type'"""

plt.figure(figsize=(15,8))

plt.subplot(1,2,1)
ax1 = sns.countplot(x = fr)
plt.title('Fraud Transactions')

plt.subplot(1,2,2)
ax2 = sns.countplot(x = va)
plt.title('Valid Transactions')

"""### Visualization of transaction counts categorized by their 'type'"""

plt.figure(figsize=(15,8))
g = sns.countplot(data=data, x='type', palette='dark',alpha=0.6)
plt.title("Counts for each Transaction types")

#finding counts
for p in g.patches:
    g.annotate(p.get_height(), (p.get_x()+0.24, p.get_height()+10000))

"""### Visualization of transaction counts categorized by their 'client_type'"""

plt.figure(figsize=(15,8))
g = sns.countplot(data=data, x='client_type', palette='dark',alpha=0.6)
plt.title("Checking for fraud occurances for each Transaction Client types")

#finding counts
for p in g.patches:
    g.annotate(p.get_height(), (p.get_x()+0.325, p.get_height()+10000))

"""### Visualization of Fraud vs Valid transaction counts categorized by their 'client_type'"""

plt.figure(figsize=(15,8))
g = sns.countplot(data=data, x='client_type', hue='isFraud', palette='dark',alpha=0.6)
plt.title("Checking for fraud occurances for each Transaction Client types")

#finding counts
for p in g.patches:
    g.annotate(p.get_height(), (p.get_x()+0.13, p.get_height()+10000))

"""## Data Preprocessing

### Importing necessary libraries
"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

"""### Obtaining one-hot encodings for 'type' column"""

one_hot_type = pd.get_dummies(data['type'])

one_hot_type

"""### Adding one-hot encoding for 'type' and removing 'type' column from data"""

data = data.drop('type', axis=1)

data = data.join(one_hot_type)

data.head()

"""### Obtaining one-hot encodings for 'client_type'one_hot_type = pd.get_dummies(data['type']) column"""

one_hot_client_type = pd.get_dummies(data['client_type'])

one_hot_client_type

"""### Adding one-hot encoding for 'client_type' and removing 'client_type' column from data"""

data = data.drop('client_type', axis=1)

data = data.join(one_hot_client_type)

data.head()

"""### Obtaining X and y"""

y = data.isFraud.fillna(value = 0)
X = data.drop('isFraud', axis=1).fillna(value = 0)
print("X:")
print(X.head())
print("y:")
print(y.head())

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, stratify=y)

"""### Normalizing X_train and X-test"""

sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

"""## Machine Learning

### Importing necessary libraries
"""

from sklearn.ensemble import RandomForestClassifier
from lightgbm.sklearn import LGBMClassifier
import xgboost as xgb
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import roc_auc_score
from sklearn.metrics import hamming_loss

"""### Creating list of ML models"""

xgbc = xgb.XGBClassifier(max_depth=3, n_jobs=-1, random_state=42, learning_rate=0.1)
rfc = RandomForestClassifier(n_estimators=15, n_jobs=-1, random_state=42)
lgbmc = LGBMClassifier(boosting_type='gbdt', objective='binary', random_sate=8888)
logreg = LogisticRegression(solver='liblinear', random_state=42)

clsrs = []
clsrs.append(xgbc)
clsrs.append(rfc)
clsrs.append(lgbmc)
clsrs.append(logreg)

"""### Training ML models"""

for clsr in clsrs:
    clsr.fit(X_train, y_train)

"""### Obtaining prediction and accuracy (hml and auc) calculations"""

hml_list = []
auc_list = []

for clsr in clsrs:
    y_pred = clsr.predict(X_test)
    y_prob = clsr.predict_proba(X_test)[:,-1]
    hml_list.append(hamming_loss(y_test, y_pred))
    auc_list.append(roc_auc_score(y_test, y_prob))

"""### Creating accuracy dictionaries"""

hml_dict = {}
auc_dict = {}
keys = ['XGBoost', 'Random Forest', 'Light GBM', 'LR']

for i in range(len(keys)):
    key = keys[i]
    hml_dict[key] = hml_list[i]
    auc_dict[key] = auc_list[i]

hml_dict_s = dict(sorted(hml_dict.items(), key=lambda item: item[1]))
hml_dict_s

auc_dict_s = dict(sorted(auc_dict.items(), key=lambda item: item[1]))
auc_dict_s

"""### Plotting AUC accuracy scores (Higher AUC score indicates better result)"""

import plotly
plotly.io.renderers.default = 'png'
plotly.io.renderers.default = 'jupyterlab'

fig = px.bar(x=list(auc_dict_s.keys()), y=list(auc_dict_s.values()), text=np.round(list(auc_dict_s.values()), 6),
            title='Accuracy (AUC) score of each Classifier',color=list(auc_dict_s.keys()),
            color_discrete_sequence=px.colors.sequential.matter)
fig.show()

"""### Plotting Accuracy (Hamming Loss) scores (Lower score indicates better results)"""

fig = px.bar(x=list(hml_dict_s.keys()), y=list(hml_dict_s.values()), text=np.round(list(hml_dict_s.values()), 6),
            title='Accuracy (Hamming Loss) score of each Classifier',color=list(hml_dict_s.keys()),
            color_discrete_sequence=px.colors.sequential.matter)
fig.show()

"""### Insights

<ul>
    <li><b>Logistic Regression</b> has the <b>best</b> performance with the - </li>
    <ul>
        <li>best (highest) AUC score - 0.983745</li>
        <li>best (lowest) Hammington Loss score - 0.000834</li>
    </ul>
    <li><b>Light Gradient-Boosting Machine</b> has <b>worst</b> performance with the - </li>
    <ul>
        <li>worst (lowest) AUC score - 0.493639</li>
        <li>worst (highest) Hamming Loss score - 0.00142</li>
    </ul>

## Neural Network

#### Importing Libraries
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

"""#### Creating Network"""

print(X_train.shape)

dnn = Sequential()
dnn.add(Dense(16, input_shape=(12,), activation='relu'))
dnn.add(Dense(32, activation='relu'))
dnn.add(Dense(8, activation='relu'))
dnn.add(Dense(1, activation='sigmoid'))

print(dnn.summary())

"""#### Compile and Train network"""

dnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

dnn.fit(X_train, y_train, epochs=15, batch_size=250, verbose=1, validation_split=0.2)

"""#### Finding DNN network predictions and prediction probabilities"""

y_prob = dnn.predict(X_test)

y_pred = []
for y_p in y_prob:
    if y_p >= 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)

y_pred[0:5]

y_prob[0:5]

"""#### Calculating accuracy scores (AUC and Hamming Loss)"""

dnn_auc = roc_auc_score(y_test, y_prob)
print(f"AUC score is: {dnn_auc}")

dnn_hml = hamming_loss(y_test, y_pred)
print(f"Hamming Loss is: {dnn_hml}")

"""### Radial Neural Network (RNN)

#### Importing Libraries
"""

from tensorflow.keras.layers import SimpleRNN

"""#### Reshaping X_train and X_test for use with RNN"""

X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_rnn = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
print(f"X_train shape is: {X_train_rnn.shape}, X_test shape is: {X_test_rnn.shape}, y_train shape is: {y_train.shape}, y_test shape is: {y_test.shape}")

"""#### Creating Network"""

rnn = Sequential()
rnn.add(SimpleRNN(16, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2]), return_sequences=True))
rnn.add(SimpleRNN(32))
rnn.add(Dense(16, activation='relu'))
rnn.add(Dense(1, activation='sigmoid'))

print(rnn.summary())

"""#### Compile and Train Network"""

rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

rnn.fit(X_train_rnn, y_train, epochs=15, batch_size=250, verbose=1, validation_split=0.2)

"""#### Finding RNN network predictions and prediction probabilities"""

y_prob = rnn.predict(X_test_rnn)

y_pred = []
for y_p in y_prob:
    if y_p >= 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)

y_pred[0:5]

y_prob[0:5]

"""#### Calculating accuracy scores (AUC and Hamming Loss)"""

rnn_auc = roc_auc_score(y_test, y_prob)
print(f"AUC score is: {rnn_auc}")

rnn_hml = hamming_loss(y_test, y_pred)
print(f"Hamming Loss is: {rnn_hml}")

"""### Comparing Neural Network accuracy scores"""

keys = ['DNN', 'RNN']

NN_auc_scrs = []
NN_auc_scrs.append(dnn_auc)
NN_auc_scrs.append(rnn_auc)

NN_hml_scrs = []
NN_hml_scrs.append(dnn_hml)
NN_hml_scrs.append(rnn_hml)

NN_auc_dict = {}
NN_hml_dict = {}

for i in range(len(keys)):
    key = keys[i]
    NN_auc_dict[key] = NN_auc_scrs[i]
    NN_hml_dict[key] = NN_hml_scrs[i]

NN_auc_dict

NN_hml_dict

"""#### Plotting AUC accuracy scores (Higher AUC score indicates better result)"""

fig = px.bar(x=list(NN_auc_dict.keys()), y=list(NN_auc_dict.values()), text=np.round(list(NN_auc_dict.values()), 6),
            title='Accuracy (AUC) score of each Classifier',color=list(NN_auc_dict.keys()),
            color_discrete_sequence=px.colors.sequential.matter)
fig.show()

"""### Plotting Accuracy (Hamming Loss) scores (Lower score indicates better results)"""

fig = px.bar(x=list(NN_hml_dict.keys()), y=list(NN_hml_dict.values()), text=np.round(list(NN_hml_dict.values()), 6),
            title='Accuracy (Hamming Loss) score of each Classifier',color=list(NN_hml_dict.keys()),
            color_discrete_sequence=px.colors.sequential.matter)
fig.show()

"""### Insights

<ul>
    <li><b>RNN</b> performs better than <b>DNN</b> - </li>
    <ul>
        <li>RNN has the better (higher) AUC score </li>
        <li>RNN has the better (lower) Hammington Loss score </li>

## Combining ML and NN results

### AUC score case
"""

auc_comb_dict = auc_dict
auc_comb_dict.update(NN_auc_dict)

auc_comb_dict_s = dict(sorted(auc_comb_dict.items(), key=lambda item: item[1]))
auc_comb_dict_s

"""### Hammington Loss score case"""

hml_comb_dict = hml_dict
hml_comb_dict.update(NN_hml_dict)

hml_comb_dict_s = dict(sorted(hml_comb_dict.items(), key=lambda item: item[1]))
hml_comb_dict_s

"""### Visualizing combined accuracy score results

#### Plotting AUC accuracy scores (Higher AUC score indicates better result)
"""

fig = px.bar(x=list(auc_comb_dict_s.keys()), y=list(auc_comb_dict_s.values()), text=np.round(list(auc_comb_dict_s.values()), 6),
            title='Accuracy (AUC) score of each Classifier',color=list(auc_comb_dict_s.keys()),
            color_discrete_sequence=px.colors.sequential.matter)
fig.show()

fig = px.bar(x=list(hml_comb_dict_s.keys()), y=list(hml_comb_dict_s.values()), text=np.round(list(hml_comb_dict_s.values()), 6),
            title='Accuracy (Hammington Loss) score of each Classifier',color=list(hml_comb_dict_s.keys()),
            color_discrete_sequence=px.colors.sequential.matter)
fig.show()